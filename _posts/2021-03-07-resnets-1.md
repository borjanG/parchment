---
layout: post
title: "ResNets and Neural ODEs"
tags: [math, cs]
---

It is superfluous to state the impact deep (machine) learning has had on modern technology, as it powers many tools of modern society, ranging from web searches to content filtering on social networks [1]. It is also increasingly present in consumer products such as cameras, smartphones and automobiles. 

From a mathematical point of view however, a large number of the employed models and techniques remain rather ad hoc.

## ResNets

Neural networks are the workhorse models of deep learning, and the main reason behind the (perhaps unreasonable) effectiveness of the latter in practice.
**Residual neural networks** (ResNets) are a specific, compound neural network architecture which has shown state of the art performance on a variety of image processing tasks.
In fact, I recently came accross a tweet stating that the original paper on ResNets ([3]) is the most cited paper (per Google Scholar) in all scientific disciplines of the 2010s. 

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">2020&#39;s most cited paper across all scientific disciplines was (still) 2015&#39;s ResNet. <br><br>Paper: <a href="https://t.co/mGGX1OpseY">https://t.co/mGGX1OpseY</a> (v/MSFTResearch) <br><br>More: <a href="https://t.co/e3Va7P6Ea8">https://t.co/e3Va7P6Ea8</a> (<a href="https://twitter.com/TDataScience?ref_src=twsrc%5Etfw">@TDataScience</a>)<a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/BestOf2020?src=hash&amp;ref_src=twsrc%5Etfw">#BestOf2020</a><a href="https://twitter.com/hashtag/WednesdayWisdom?src=hash&amp;ref_src=twsrc%5Etfw">#WednesdayWisdom</a> <a href="https://t.co/fdAZPXABZV">pic.twitter.com/fdAZPXABZV</a></p>&mdash; MIT CSAIL (@MIT_CSAIL) <a href="https://twitter.com/MIT_CSAIL/status/1334192858635505665?ref_src=twsrc%5Etfw">December 2, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

Let us recall that, in supervised learning, we are interested in approximating a function 

$$
f: \mathcal{X} \to \mathcal{Y}
$$ 

which is unknown a priori, and we only dispose of a dataset of $N$ distinct points:

$$
\{x_i, y_i \}_{i=1}^N.
$$

Here $x_i\in\mathcal{X}\subset\mathbf{R}^d$ are the inputs, whereas $y_i\in\mathcal{Y}$ are the labels. We distinguish two different tasks depending on the nature of the labels: classification ($\mathcal{Y}$ is a finite subset of $\mathbf{N}$, e.g. {$1,\ldots,m$} for $m$ classes), and regression ($\mathcal{Y}$ is a subset of $\mathbf{R}^m$ consisting of continuous floating-point variables).

We thus approximate $f$ by using the "flow"-map $x_i\mapsto P\mathbf{x}_i^{L}$ of the ResNet, which takes the form of a discrete-time dynamical system

$$
\begin{cases}
\mathbf{x}_i^{k+1} = \mathbf{x}_i^k + \sigma(w^k \mathbf{x}_i^k + b^k) &\text{ for } k \in \{0, \ldots, L-1\} \\
\mathbf{x}_i^0 = x_i.
\end{cases}
$$

Here $P$ is an affine map which plays the role of matching dimensions, as to be able to compare the ResNet output $\mathbf{x}_i^{L}$ (which is $d$-dimensional) with the corresponding label $y_i$. 
The architecture we defined above may be tweaked to the user's liking, for instance, by adding supplementary weight or bias matrices and further compositional nonlinearities. We stick to this simple case for didactics.

For presentational simplicity, let us concentrate on classification tasks, and thus consider $\mathcal{Y} =$ {$1,\ldots,m$}. 
The main practical challenge of ML in **training** the ResNet, namely computing parameters $\theta=${$w^k, b^k$} which minimize the (regularized) empirical risk (we focus on cross-entropy loss and weight-decay reg.)

$$
-\frac{1}{N} \sum_{i=1}^N \log\Big(\frac{e^{y_i P\mathbf{x}_i^{L}}}{\sum_{j=1}^m e^{y_j P\mathbf{x}_i^L}}\Big) + \lambda\|\theta\|^2.
$$

## Neural ODEs

Work in progress (coming soon)..

<center>
<div class="landscape">
<img src="../assets/posts/1/trajectory.mp4">
</div>
<div class="landscape">
<img src="../assets/posts/1/generalization.pdf">
</div>
</center>

## References.

[1] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature,
521(7553):436–444.

[2] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778.

<!-- [5] Léon Bottou, Frank E. Curtis and Jorge Nocedal: Optimization Methods for Large-Scale Machine Learning, Siam Review, 60(2):223-311, 2018.

[8] Weinan, E. (2017). A proposal on machine learning via dynamical systems. Communications in Mathematics and Statistics, 5(1):1–11. -->